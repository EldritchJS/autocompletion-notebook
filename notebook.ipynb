{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probabilistic classifier.\n",
    "Given $n$ _attributes_, that is\n",
    "\n",
    "$$\n",
    "\\text{attributes} = \\lbrace{a_1, a_2, a_3, \\dots, a_n \\rbrace},\n",
    "$$\n",
    "\n",
    "and a set of $k$ _labels_, we can calculate the conditional probability of each label given a certain set of attributes. That is:\n",
    "\n",
    "$$\n",
    "p\\left(L_k|a_1, a_2, a_3,\\dots,a_n\\right)\n",
    "$$.\n",
    "\n",
    "This is done by direct application of Baye's theorem:\n",
    "\n",
    "$$\n",
    "p\\left(L_k|a_1,a_2,a_3,\\dots,a_n\\right) = \\frac{p\\left(L_k\\right)p\\left(a_1,a_2,a_3,\\dots,a_n|L_k\\right)}{p\\left(a_1,a_2,a_3,\\dots,a_n\\right)}\n",
    "$$\n",
    "\n",
    "## Example\n",
    "\n",
    "For the following minimal example a Java implementation of the Naive Bayes algorithm will be used (available at https://github.com/ruivieira/java-naive-bayes).\n",
    "\n",
    "Consider a simple IT purchasing system where the user can choose a laptop brand (`Apple` or `Lenovo`) for use in a specific department (`design` or `accounting`) in one of two offices (`US` or `UK`).\n",
    "\n",
    "In this case, NB will try to classify the laptop brand according to the historical purchase data. It is clear then that the attributes will be:\n",
    "\n",
    "$$\n",
    "    a = \\lbrace\\text{user}, \\text{department}, \\text{office}\\rbrace\n",
    "$$\n",
    "\n",
    "and the labels will be\n",
    "\n",
    "$$\n",
    "    L = \\lbrace\\text{Brand A}, \\text{Brand B}\\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven org.ruivieira:naivebayes:0.1-SNAPSHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.ruivieira.ml.naivebayes.NaiveBayes;\n",
    "import org.ruivieira.ml.naivebayes.Model;\n",
    "\n",
    "import java.util.Map;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model model = Model.create();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by adding the first record. User Anna buys an Apple for the US design department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(new String[]{\"Anna\", \"design\", \"US\"}, \"Apple\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to predict that label (outcome) for any of the attributes, the result will be unsurprising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna: {Apple=100.0}\n",
      "design: {Apple=100.0}\n",
      "US: {Apple=100.0}\n"
     ]
    }
   ],
   "source": [
    "NaiveBayes naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Anna: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"US: \" + naiveBayes.classify(new String[]{\"US\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add a purchase for a Lenovo for US accounting department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna: {Lenovo=50.0, Apple=50.0}\n",
      "design: {Lenovo=5.0E-9, Apple=50.0}\n",
      "US: {Lenovo=50.0, Apple=50.0}\n"
     ]
    }
   ],
   "source": [
    "model.train(new String[]{\"Anna\", \"accounting\", \"US\"}, \"Lenovo\");\n",
    "naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Anna: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"US: \" + naiveBayes.classify(new String[]{\"US\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing we couldn't figure out ourselves, yet. Anna is 50/50 as likely to buy a Lenovo or an Apple. The design department is more likely to get an Apple (50% vs. ~0%) and the US office is as likely to get a Lenovo or an Apple.\n",
    "\n",
    "Let's now add a third user, Bill. Bill makes the same purchasing choices as Anna, but he also buys for the UK office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(new String[]{\"Bill\", \"accounting\", \"US\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Bill\", \"design\", \"US\"}, \"Apple\");\n",
    "model.train(new String[]{\"Bill\", \"accounting\", \"UK\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Bill\", \"design\", \"UK\"}, \"Apple\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna: {Lenovo=16.666666666666664, Apple=16.666666666666664}\n",
      "Bill: {Lenovo=16.666666666666664, Apple=16.666666666666664}\n",
      "design: {Lenovo=5.0E-9, Apple=50.0}\n",
      "US: {Lenovo=33.33333333333333, Apple=33.33333333333333}\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Anna: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"Bill: \" + naiveBayes.classify(new String[]{\"Anna\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"US: \" + naiveBayes.classify(new String[]{\"US\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still nothing surprising. However, one of the strengths of NB is the ability to combine attributes to get insights. Let's see that adding another user, Claire. Claire will buy Lenovos for all the offices and departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(new String[]{\"Claire\", \"accounting\", \"US\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Claire\", \"design\", \"US\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Claire\", \"accounting\", \"UK\"}, \"Lenovo\");\n",
    "model.train(new String[]{\"Claire\", \"design\", \"UK\"}, \"Lenovo\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claire: {Lenovo=40.0, Apple=3.0E-9}\n",
      "design: {Lenovo=20.0, Apple=30.0}\n",
      "design US: {Lenovo=11.428571428571427, Apple=20.0}\n",
      "Bill accounting: {Lenovo=70.0, Apple=30.0}\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = new NaiveBayes(model);\n",
    "System.out.println(\"Claire: \" + naiveBayes.classify(new String[]{\"Claire\"}).toString());\n",
    "System.out.println(\"design: \" + naiveBayes.classify(new String[]{\"design\"}).toString());\n",
    "System.out.println(\"design US: \" + naiveBayes.classify(new String[]{\"design\", \"US\"}).toString());\n",
    "System.out.println(\"Bill accounting: \" + naiveBayes.classify(new String[]{\"Bill accounting\"}).toString());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to see the usefulness of combining attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "A classifier consisting of logical rules for traversing a tree from its root to a leaf node, with attributes of the entity modeled serving as predicates for the aforementioned logical rules. The leaf nodes then correspond to the class to which the given input is predicted.\n",
    "\n",
    "Example of a tree\n",
    "TBC\n",
    "\n",
    "Building a tree\n",
    "TBC\n",
    "\n",
    "Splits\n",
    "TBC\n",
    "\n",
    "Cost functions\n",
    "TBC\n",
    "\n",
    "Gini index\n",
    "TBC\n",
    "\n",
    "With an idea of assessing the quality of splits, there remains a need for knowing when to stop splitting. Naturally, splitting could continue until all training vectors are mapped to a leaf node. It will be discussed below why this is disadvantageous. Other criteria can be utilized, such as a maximum tree depth or width, or a threshold on cost function. \n",
    "\n",
    "## Advantages\n",
    "**Intuitive** Assuming my prose is up to snuff, the architecture of the classifier should be apparent to anyone completing an intro data structures class. The resulting implementation is also accessible to anyone with basic programming knowledge: it's all ifs and elses!\n",
    "\n",
    "**Data flexibility** They can work with numerical and categorical data with ease; whereas other classification methods will at least require a one hot encoding of categorical variables into n binary variables where n is the number of possible values the variable can take. For example the category of department in this notebook's example would require at least two features: namely \"design\" and \"accounting\".\n",
    "\n",
    "## Disadvantages\n",
    "**Overfitting** That is, the situation arising as a result of creating a classifier which works very well for its training data but is inapplicable to input received in the wild. This can be a significant detriment to tree methods, as the training approach lends itself to human bias. \n",
    "\n",
    "**Heuristic-driven** While some techniques in machine learning can provide confidence on solution optimality, tree methods are not one of them. Intuition acquired via experience can help ameliorate this situation, however, in many cases grid search methods will be necessary to be certain of a tree's appropriateness. \n",
    "\n",
    "## Improving via ensembles\n",
    "One way to improve upon the disadvantages mentioned above should be accessible to programs: namely, adding a level of indirection/expansion to the classifier architecture. By utilizing a collection, or ensemble, of trees that then contribute to a final classification, a more robust classifier is made manifest. \n",
    "\n",
    "### Random Forests\n",
    "\n",
    "In a random forest, a subset of the training data is selected, from which a decision tree is generated. TBC\n",
    "\n",
    "## Example\n",
    "\n",
    "The same minimal example as the NB case above will be used, with a Java implementation of a Decision Tree algorithm (available at https://github.com/ruivieira/java-decision-tree).\n",
    "\n",
    "Consider a simple IT purchasing system where the user can choose a laptop brand (`Apple` or `Lenovo`) for use in a specific department (`design` or `accounting`) in one of two offices (`US` or `UK`).\n",
    "\n",
    "In this case, NB will try to classify the laptop brand according to the historical purchase data. It is clear then that the attributes will be:\n",
    "\n",
    "$$\n",
    "    a = \\lbrace\\text{user}, \\text{department}, \\text{office}\\rbrace\n",
    "$$\n",
    "\n",
    "and the labels will be\n",
    "\n",
    "$$\n",
    "    L = \\lbrace\\text{Brand A}, \\text{Brand B}\\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven org.ruivieira:decisiontree:0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.ruivieira.ml.decisiontree.DecisionTree;\n",
    "import org.ruivieira.ml.decisiontree.TreeConfig;\n",
    "import org.ruivieira.ml.decisiontree.Dataset;\n",
    "import org.ruivieira.ml.decisiontree.Item;\n",
    "import org.ruivieira.ml.decisiontree.features.StringValue;\n",
    "\n",
    "\n",
    "import java.util.Map;\n",
    "\n",
    "TreeConfig treeconfig = TreeConfig.create();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset dataset = Dataset.create();\n",
    "Item trainItem = Item.create();\n",
    "trainItem.add(\"Anna\", new StringValue(\"Apple\"));\n",
    "trainItem.add(\"design\", new StringValue(\"Apple\"));\n",
    "dataset.add(trainItem);\n",
    "dataset.size();\n",
    "treeconfig.setData(dataset);\n",
    "DecisionTree decisiontree = DecisionTree.create(treeconfig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TreeConfig{minCount=1, decision='category', entropyThreshold=0.01, maxDepth=70, data=org.ruivieira.ml.decisiontree.Dataset@292a4be8, ignore=[]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeconfig.toString();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringValue{data='Apple'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.valueFrequency(\"Anna\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".java",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.1+13-LTS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
